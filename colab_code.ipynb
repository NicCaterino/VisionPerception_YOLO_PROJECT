{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y1ox0o833y8"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "## 1. Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1GTYOQH33y8",
        "outputId": "2bb9b9ee-4b90-4e84-d774-da59aa614501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "import PIL.Image\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display\n",
        "from seaborn import color_palette\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbIU_XgG_QRC",
        "outputId": "d27076a5-18f6-4c2d-c250-9fea0a33e1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VISIOPE_PROJECT_caterino_murra_petroni'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 79 (delta 5), reused 0 (delta 0), pack-reused 63\u001b[K\n",
            "Unpacking objects: 100% (79/79), 47.76 MiB | 5.83 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/Luca1920342/VISIOPE_PROJECT_caterino_murra_petroni.git\n",
        "! cp -R VISIOPE_PROJECT_caterino_murra_petroni/* /content\n",
        "! rm -rf VISIOPE_PROJECT_caterino_murra_petroni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40s757qO33y-"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "## 2. Model hyperparameters\n",
        "Some configurations for Yolo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e8n4A4iU33y_"
      },
      "outputs": [],
      "source": [
        "_BATCH_NORM_DECAY = 0.9\n",
        "_BATCH_NORM_EPSILON = 1e-05\n",
        "_LEAKY_RELU = 0.1\n",
        "_ANCHORS = [(10, 13), (16, 30), (33, 23),\n",
        "            (30, 61), (62, 45), (59, 119),\n",
        "            (116, 90), (156, 198), (373, 326)]\n",
        "_MODEL_SIZE = (416, 416)\n",
        "_CONF_THRESHOLD = 0.5\n",
        "_BATCH_SIZE = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbblhPgq33zA"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "## 3. Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ul82fI33zA"
      },
      "source": [
        "### Batch norm and fixed padding\n",
        "It's useful to define `batch_norm` function since the model uses batch norms with shared parameters heavily. Also, same as ResNet, Yolo uses convolution with fixed padding, which means that padding is defined only by the size of the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HNYVpWvh33zB"
      },
      "outputs": [],
      "source": [
        "def batch_norm(inputs, training, data_format):\n",
        "    \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n",
        "    return tf.layers.batch_normalization(\n",
        "        inputs = inputs, axis=1 if data_format == 'channels_first' else 3,\n",
        "        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n",
        "        scale=True, training=training)\n",
        "\n",
        "\n",
        "def fixed_padding(inputs, kernel_size, data_format):\n",
        "    \"\"\"ResNet implementation of fixed padding.\n",
        "\n",
        "    Pads the input along the spatial dimensions independently of input size.\n",
        "\n",
        "    Args:\n",
        "        inputs: Tensor input to be padded.\n",
        "        kernel_size: The kernel to be used in the conv2d or max_pool2d.\n",
        "        data_format: The input format.\n",
        "    Returns:\n",
        "        A tensor with the same format as the input.\n",
        "    \"\"\"\n",
        "    pad_total = kernel_size - 1\n",
        "    pad_beg = pad_total // 2\n",
        "    pad_end = pad_total - pad_beg\n",
        "\n",
        "    if data_format == 'channels_first':\n",
        "        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n",
        "                                        [pad_beg, pad_end],\n",
        "                                        [pad_beg, pad_end]])\n",
        "    else:\n",
        "        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
        "                                        [pad_beg, pad_end], [0, 0]])\n",
        "    return padded_inputs\n",
        "\n",
        "\n",
        "def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n",
        "    \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
        "    if strides > 1:\n",
        "        inputs = fixed_padding(inputs, kernel_size, data_format)\n",
        "\n",
        "    return tf.layers.conv2d(\n",
        "        inputs = inputs, filters=filters, kernel_size=kernel_size,\n",
        "        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n",
        "        use_bias=False, data_format=data_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY0IbqVa33zB"
      },
      "source": [
        "### Feature extraction: Darknet-53\n",
        "For feature extraction Yolo uses Darknet-53 neural net pretrained on ImageNet. Same as ResNet,  Darknet-53 has shortcut (residual) connections, which help information from earlier layers flow further. We omit the last 3 layers (Avgpool, Connected and Softmax) since we only need the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x6lSGeNP33zB"
      },
      "outputs": [],
      "source": [
        "def darknet53_residual_block(inputs, filters, training, data_format,\n",
        "                             strides=1):\n",
        "    \"\"\"Creates a residual block for Darknet.\"\"\"\n",
        "    shortcut = inputs\n",
        "\n",
        "    inputs = conv2d_fixed_padding(\n",
        "        inputs, filters=filters, kernel_size=1, strides=strides,\n",
        "        data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(\n",
        "        inputs, filters=2 * filters, kernel_size=3, strides=strides,\n",
        "        data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    inputs += shortcut\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def darknet53(inputs, training, data_format):\n",
        "    \"\"\"Creates Darknet53 model for feature extraction.\"\"\"\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3,\n",
        "                                  data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3,\n",
        "                                  strides=2, data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    inputs = darknet53_residual_block(inputs, filters=32, training=training,\n",
        "                                      data_format=data_format)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3,\n",
        "                                  strides=2, data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    for _ in range(2):\n",
        "        inputs = darknet53_residual_block(inputs, filters=64,\n",
        "                                          training=training,\n",
        "                                          data_format=data_format)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3,\n",
        "                                  strides=2, data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    for _ in range(8):\n",
        "        inputs = darknet53_residual_block(inputs, filters=128,\n",
        "                                          training=training,\n",
        "                                          data_format=data_format)\n",
        "\n",
        "    route1 = inputs\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3,\n",
        "                                  strides=2, data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    for _ in range(8):\n",
        "        inputs = darknet53_residual_block(inputs, filters=256,\n",
        "                                          training=training,\n",
        "                                          data_format=data_format)\n",
        "\n",
        "    route2 = inputs\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3,\n",
        "                                  strides=2, data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    for _ in range(4):\n",
        "        inputs = darknet53_residual_block(inputs, filters=512,\n",
        "                                          training=training,\n",
        "                                          data_format=data_format)\n",
        "\n",
        "    return route1, route2, inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVp7v1Or33zC"
      },
      "source": [
        "### Convolution layers\n",
        "Yolo has a large number of convolutional layers. It's useful to group them in blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6zKOFk9T33zC"
      },
      "outputs": [],
      "source": [
        "def yolo_convolution_block(inputs, filters, training, data_format):\n",
        "    \"\"\"Creates convolution operations layer used after Darknet.\"\"\"\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
        "                                  data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
        "                                  data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
        "                                  data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
        "                                  data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
        "                                  data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    route = inputs\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
        "                                  data_format=data_format)\n",
        "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "    return route, inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8huAq8t33zC"
      },
      "source": [
        "### Detection layers\n",
        "Yolo has 3 detection layers, that detect on 3 different scales using respective anchors. For each cell in the feature map the detection layer predicts `n_anchors * (5 + n_classes)` values using 1x1 convolution. For each scale we have `n_anchors = 3`. `5 + n_classes` means that respectively to each of 3 anchors we are going to predict 4 coordinates of the box, its confidence score (the probability of containing an object) and class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XBSVXhmS33zD"
      },
      "outputs": [],
      "source": [
        "def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n",
        "    \"\"\"Creates Yolo final detection layer.\n",
        "\n",
        "    Detects boxes with respect to anchors.\n",
        "\n",
        "    Args:\n",
        "        inputs: Tensor input.\n",
        "        n_classes: Number of labels.\n",
        "        anchors: A list of anchor sizes.\n",
        "        img_size: The input size of the model.\n",
        "        data_format: The input format.\n",
        "\n",
        "    Returns:\n",
        "        Tensor output.\n",
        "    \"\"\"\n",
        "    n_anchors = len(anchors)\n",
        "\n",
        "    inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n",
        "                              kernel_size=1, strides=1, use_bias=True,\n",
        "                              data_format=data_format)\n",
        "\n",
        "    shape = inputs.get_shape().as_list()\n",
        "    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n",
        "    if data_format == 'channels_first':\n",
        "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
        "    inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1],\n",
        "                                 5 + n_classes])\n",
        "\n",
        "    strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n",
        "\n",
        "    box_centers, box_shapes, confidence, classes = \\\n",
        "        tf.split(inputs, [2, 2, 1, n_classes], axis=-1)\n",
        "\n",
        "    x = tf.range(grid_shape[0], dtype=tf.float32)\n",
        "    y = tf.range(grid_shape[1], dtype=tf.float32)\n",
        "    x_offset, y_offset = tf.meshgrid(x, y)\n",
        "    x_offset = tf.reshape(x_offset, (-1, 1))\n",
        "    y_offset = tf.reshape(y_offset, (-1, 1))\n",
        "    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n",
        "    x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n",
        "    x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n",
        "    box_centers = tf.nn.sigmoid(box_centers)\n",
        "    box_centers = (box_centers + x_y_offset) * strides\n",
        "\n",
        "    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1])\n",
        "    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n",
        "\n",
        "    confidence = tf.nn.sigmoid(confidence)\n",
        "\n",
        "    classes = tf.nn.sigmoid(classes)\n",
        "\n",
        "    inputs = tf.concat([box_centers, box_shapes,\n",
        "                        confidence, classes], axis=-1)\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugAvTr7333zD"
      },
      "source": [
        "### Upsample layer\n",
        "In order to concatenate with shortcut outputs from Darknet-53 before applying detection on a different scale, we are going to upsample the feature map using nearest neighbor interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Uv04mnMU33zG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def upsample(inputs, out_shape, data_format):\n",
        "    \"\"\"Upsamples to `out_shape` using nearest neighbor interpolation.\"\"\"\n",
        "    if data_format == 'channels_first':\n",
        "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
        "        new_height = out_shape[3]\n",
        "        new_width = out_shape[2]\n",
        "    else:\n",
        "        new_height = out_shape[2]\n",
        "        new_width = out_shape[1]\n",
        "\n",
        "    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n",
        "\n",
        "    if data_format == 'channels_first':\n",
        "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4XEhat033zH"
      },
      "source": [
        "### Non-max suppression\n",
        "The model is going to produce a lot of boxes, so we need a way to discard the boxes with low confidence scores. Also, to avoid having multiple boxes for one object, we will discard the boxes with high overlap as well using non-max suppresion for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kMcCE8Hp33zH"
      },
      "outputs": [],
      "source": [
        "def build_boxes(inputs):\n",
        "    \"\"\"Computes top left and bottom right points of the boxes.\"\"\"\n",
        "    center_x, center_y, width, height, confidence, classes = \\\n",
        "        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n",
        "\n",
        "    top_left_x = center_x - width / 2\n",
        "    top_left_y = center_y - height / 2\n",
        "    bottom_right_x = center_x + width / 2\n",
        "    bottom_right_y = center_y + height / 2\n",
        "\n",
        "    boxes = tf.concat([top_left_x, top_left_y,\n",
        "                       bottom_right_x, bottom_right_y,\n",
        "                       confidence, classes], axis=-1)\n",
        "\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,\n",
        "                        confidence_threshold):\n",
        "    \"\"\"Performs non-max suppression separately for each class.\n",
        "\n",
        "    Args:\n",
        "        inputs: Tensor input.\n",
        "        n_classes: Number of classes.\n",
        "        max_output_size: Max number of boxes to be selected for each class.\n",
        "        iou_threshold: Threshold for the IOU.\n",
        "        confidence_threshold: Threshold for the confidence score.\n",
        "    Returns:\n",
        "        A list containing class-to-boxes dictionaries\n",
        "            for each sample in the batch.\n",
        "    \"\"\"\n",
        "    batch = tf.unstack(inputs)\n",
        "    boxes_dicts = []\n",
        "    for boxes in batch:\n",
        "        boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold)\n",
        "        classes = tf.argmax(boxes[:, 5:], axis=-1)\n",
        "        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n",
        "        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n",
        "\n",
        "        boxes_dict = dict()\n",
        "        for cls in range(n_classes):\n",
        "            mask = tf.equal(boxes[:, 5], cls)\n",
        "            mask_shape = mask.get_shape()\n",
        "            if mask_shape.ndims != 0:\n",
        "                class_boxes = tf.boolean_mask(boxes, mask)\n",
        "                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n",
        "                                                              [4, 1, -1],\n",
        "                                                              axis=-1)\n",
        "                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n",
        "                indices = tf.image.non_max_suppression(boxes_coords,\n",
        "                                                       boxes_conf_scores,\n",
        "                                                       max_output_size,\n",
        "                                                       iou_threshold)\n",
        "                class_boxes = tf.gather(class_boxes, indices)\n",
        "                boxes_dict[cls] = class_boxes[:, :5]\n",
        "\n",
        "        boxes_dicts.append(boxes_dict)\n",
        "\n",
        "    return boxes_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxopTGwO33zH"
      },
      "source": [
        "### Final model class\n",
        "Finally, let's define the model class using all of the layers described previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A8RPHF5v33zI"
      },
      "outputs": [],
      "source": [
        "class Yolo_v3:\n",
        "    \"\"\"Yolo v3 model class.\"\"\"\n",
        "\n",
        "    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n",
        "                 confidence_threshold, data_format=None):\n",
        "        \"\"\"Creates the model.\n",
        "\n",
        "        Args:\n",
        "            n_classes: Number of class labels.\n",
        "            model_size: The input size of the model.\n",
        "            max_output_size: Max number of boxes to be selected for each class.\n",
        "            iou_threshold: Threshold for the IOU.\n",
        "            confidence_threshold: Threshold for the confidence score.\n",
        "            data_format: The input format.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        if not data_format:\n",
        "            if tf.test.is_built_with_cuda():\n",
        "                data_format = 'channels_first'\n",
        "            else:\n",
        "                data_format = 'channels_last'\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.model_size = model_size\n",
        "        self.max_output_size = max_output_size\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.data_format = data_format\n",
        "\n",
        "    def __call__(self, inputs, training):\n",
        "        \"\"\"Add operations to detect boxes for a batch of input images.\n",
        "\n",
        "        Args:\n",
        "            inputs: A Tensor representing a batch of input images.\n",
        "            training: A boolean, whether to use in training or inference mode.\n",
        "\n",
        "        Returns:\n",
        "            A list containing class-to-boxes dictionaries\n",
        "                for each sample in the batch.\n",
        "        \"\"\"\n",
        "        with tf.variable_scope('yolo_v3_model'):\n",
        "            if self.data_format == 'channels_first':\n",
        "                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
        "\n",
        "            inputs = inputs / 255\n",
        "\n",
        "            route1, route2, inputs = darknet53(inputs, training=training,\n",
        "                                               data_format=self.data_format)\n",
        "\n",
        "            route, inputs = yolo_convolution_block(\n",
        "                inputs, filters=512, training=training,\n",
        "                data_format=self.data_format)\n",
        "            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n",
        "                                 anchors=_ANCHORS[6:9],\n",
        "                                 img_size=self.model_size,\n",
        "                                 data_format=self.data_format)\n",
        "\n",
        "            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n",
        "                                          data_format=self.data_format)\n",
        "            inputs = batch_norm(inputs, training=training,\n",
        "                                data_format=self.data_format)\n",
        "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "            upsample_size = route2.get_shape().as_list()\n",
        "            inputs = upsample(inputs, out_shape=upsample_size,\n",
        "                              data_format=self.data_format)\n",
        "            axis = 1 if self.data_format == 'channels_first' else 3\n",
        "            inputs = tf.concat([inputs, route2], axis=axis)\n",
        "            route, inputs = yolo_convolution_block(\n",
        "                inputs, filters=256, training=training,\n",
        "                data_format=self.data_format)\n",
        "            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n",
        "                                 anchors=_ANCHORS[3:6],\n",
        "                                 img_size=self.model_size,\n",
        "                                 data_format=self.data_format)\n",
        "\n",
        "            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n",
        "                                          data_format=self.data_format)\n",
        "            inputs = batch_norm(inputs, training=training,\n",
        "                                data_format=self.data_format)\n",
        "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "            upsample_size = route1.get_shape().as_list()\n",
        "            inputs = upsample(inputs, out_shape=upsample_size,\n",
        "                              data_format=self.data_format)\n",
        "            inputs = tf.concat([inputs, route1], axis=axis)\n",
        "            route, inputs = yolo_convolution_block(\n",
        "                inputs, filters=128, training=training,\n",
        "                data_format=self.data_format)\n",
        "            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n",
        "                                 anchors=_ANCHORS[0:3],\n",
        "                                 img_size=self.model_size,\n",
        "                                 data_format=self.data_format)\n",
        "\n",
        "            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n",
        "\n",
        "            inputs = build_boxes(inputs)\n",
        "\n",
        "            boxes_dicts = non_max_suppression(\n",
        "                inputs, n_classes=self.n_classes,\n",
        "                max_output_size=self.max_output_size,\n",
        "                iou_threshold=self.iou_threshold,\n",
        "                confidence_threshold=self.confidence_threshold)\n",
        "\n",
        "            return boxes_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVj2vmjQ33zI"
      },
      "source": [
        "<a id=\"4\"></a>\n",
        "## 4. Utility functions\n",
        "Here are some utility functions that will help us load images as NumPy arrays, load class names from the official file and draw the predicted boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FtHEIs9C33zI"
      },
      "outputs": [],
      "source": [
        "def load_images(imgs, model_size):\n",
        "    \"\"\"Loads images in a 4D array.\n",
        "\n",
        "    Args:\n",
        "        img_names: A list of images names.\n",
        "        model_size: The input size of the model.\n",
        "        data_format: A format for the array returned\n",
        "            ('channels_first' or 'channels_last').\n",
        "\n",
        "    Returns:\n",
        "        A 4D NumPy array.\n",
        "    \"\"\"\n",
        "    result_imgs = []\n",
        "\n",
        "    for img_array in imgs:\n",
        "        #img = Image.open(img_name)\n",
        "        img = Image.fromarray(img_array)\n",
        "        img = img.resize(size=model_size)\n",
        "        img = np.array(img, dtype=np.float32)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        result_imgs.append(img)\n",
        "\n",
        "    result_imgs = np.concatenate(result_imgs)\n",
        "\n",
        "    return result_imgs\n",
        "\n",
        "\n",
        "def load_class_names(file_name):\n",
        "    \"\"\"Returns a list of class names read from `file_name`.\"\"\"\n",
        "    with open(file_name, 'r') as f:\n",
        "        class_names = f.read().splitlines()\n",
        "    return class_names\n",
        "\n",
        "\n",
        "def draw_boxes(imgs, boxes_dicts, class_names, model_size):\n",
        "    \"\"\"Draws detected boxes.\n",
        "\n",
        "    Args:\n",
        "        img_names: A list of input images names.\n",
        "        boxes_dict: A class-to-boxes dictionary.\n",
        "        class_names: A class names list.\n",
        "        model_size: The input size of the model.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n",
        "    for num, img_array, boxes_dict in zip(range(len(imgs)), imgs,\n",
        "                                         boxes_dicts):\n",
        "        #img = Image.open(img_name)\n",
        "        img = Image.fromarray(img_array)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        font = ImageFont.truetype(font='/content/futur.ttf',\n",
        "                                  size=(img.size[0] + img.size[1]) // 100)\n",
        "        resize_factor = \\\n",
        "            (img.size[0] / model_size[0], img.size[1] / model_size[1])\n",
        "\n",
        "        for cls in class_names:\n",
        "            boxes = boxes_dict[cls]\n",
        "            if np.size(boxes) != 0:\n",
        "                color = colors[cls]\n",
        "\n",
        "                for box in boxes:\n",
        "                    xy, confidence = box[:4], box[4]\n",
        "\n",
        "                    if confidence < _CONF_THRESHOLD:\n",
        "                      continue\n",
        "\n",
        "                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n",
        "                    x0, y0 = xy[0], xy[1]\n",
        "                    thickness = (img.size[0] + img.size[1]) // 200\n",
        "                    for t in np.linspace(0, 1, thickness):\n",
        "                        xy[0], xy[1] = xy[0] + t, xy[1] + t\n",
        "                        xy[2], xy[3] = xy[2] - t, xy[3] - t\n",
        "                        draw.rectangle(xy, outline=tuple(color))\n",
        "                    #text = '{} {:.1f}%'.format(class_names[cls], confidence * 100)\n",
        "                    #text_size = draw.textsize(text, font=font)\n",
        "                    #draw.rectangle([x0, y0 - text_size[1], x0 + text_size[0], y0], fill=tuple(color))\n",
        "                    #draw.text((x0, y0 - text_size[1]), text, fill='black', font=font)\n",
        "\n",
        "        result.append(img)\n",
        "        #display(img)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzPjBbSI33zJ"
      },
      "source": [
        "<a id=\"5\"></a>\n",
        "## 5. Converting weights to Tensorflow format\n",
        "Now it's time to load the official weights. We are going to iterate through the file and gradually create `tf.assign` operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IqJJ0vvM6xjl"
      },
      "outputs": [],
      "source": [
        "def load_weights(variables, file_name):\n",
        "    \"\"\"Reshapes and loads official pretrained Yolo weights.\n",
        "\n",
        "    Args:\n",
        "        variables: A list of tf.Variable to be assigned.\n",
        "        file_name: A name of a file containing weights.\n",
        "\n",
        "    Returns:\n",
        "        A list of assign operations.\n",
        "    \"\"\"\n",
        "    with open(file_name, \"rb\") as f:\n",
        "        # Skip first 5 values containing irrelevant info\n",
        "        np.fromfile(f, dtype=np.int32, count=5)\n",
        "        weights = np.fromfile(f, dtype=np.float32)\n",
        "\n",
        "        assign_ops = []\n",
        "        ptr = 0\n",
        "\n",
        "        # Load weights for Darknet part.\n",
        "        # Each convolution layer has batch normalization.\n",
        "        for i in range(52):\n",
        "            conv_var = variables[5 * i]\n",
        "            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n",
        "            batch_norm_vars = [beta, gamma, mean, variance]\n",
        "\n",
        "            for var in batch_norm_vars:\n",
        "                shape = var.shape.as_list()\n",
        "                num_params = np.prod(shape)\n",
        "                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
        "                ptr += num_params\n",
        "                assign_ops.append(tf.assign(var, var_weights))\n",
        "\n",
        "            shape = conv_var.shape.as_list()\n",
        "            num_params = np.prod(shape)\n",
        "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
        "                (shape[3], shape[2], shape[0], shape[1]))\n",
        "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
        "            ptr += num_params\n",
        "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
        "\n",
        "        # Loading weights for Yolo part.\n",
        "        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n",
        "        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n",
        "        unnormalized = [6, 13, 20]\n",
        "        for j in range(3):\n",
        "            for i in ranges[j]:\n",
        "                current = 52 * 5 + 5 * i + j * 2\n",
        "                conv_var = variables[current]\n",
        "                gamma, beta, mean, variance =  \\\n",
        "                    variables[current + 1:current + 5]\n",
        "                batch_norm_vars = [beta, gamma, mean, variance]\n",
        "\n",
        "                for var in batch_norm_vars:\n",
        "                    shape = var.shape.as_list()\n",
        "                    num_params = np.prod(shape)\n",
        "                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
        "                    ptr += num_params\n",
        "                    assign_ops.append(tf.assign(var, var_weights))\n",
        "\n",
        "                shape = conv_var.shape.as_list()\n",
        "                num_params = np.prod(shape)\n",
        "                var_weights = weights[ptr:ptr + num_params].reshape(\n",
        "                    (shape[3], shape[2], shape[0], shape[1]))\n",
        "                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
        "                ptr += num_params\n",
        "                assign_ops.append(tf.assign(conv_var, var_weights))\n",
        "\n",
        "            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n",
        "            shape = bias.shape.as_list()\n",
        "            num_params = np.prod(shape)\n",
        "            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
        "            ptr += num_params\n",
        "            assign_ops.append(tf.assign(bias, var_weights))\n",
        "\n",
        "            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n",
        "            shape = conv_var.shape.as_list()\n",
        "            num_params = np.prod(shape)\n",
        "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
        "                (shape[3], shape[2], shape[0], shape[1]))\n",
        "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
        "            ptr += num_params\n",
        "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
        "\n",
        "    return assign_ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgbE3J_o33zK"
      },
      "source": [
        "<a id=\"6\"></a>\n",
        "## 6. Running the model\n",
        "Now we can run the model using some sample images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzPsA-ad33zK"
      },
      "source": [
        "### Sample images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtjWEHqV33zK",
        "outputId": "7c7591e8-4609-441f-f915-c509509f887f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames:  436\n",
            "Frame rate:  25.0\n"
          ]
        }
      ],
      "source": [
        "video = cv2.VideoCapture('/content/input/Example1.mp4')\n",
        "frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
        "frames = []\n",
        "\n",
        "success,image = video.read()\n",
        "total_frames = 0\n",
        "while success:\n",
        "  frames.append(image)\n",
        "  success, image = video.read()\n",
        "  total_frames += 1\n",
        "\n",
        "print(\"Total frames: \", total_frames)\n",
        "print(\"Frame rate: \", frame_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPR82w5v33zK"
      },
      "source": [
        "### Detections\n",
        "Testing the model with IoU (Interception over Union ratio used in non-max suppression) threshold and confidence threshold both set to 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDf1Z3jF6ha7",
        "outputId": "656b08fb-0e0e-4231-aac8-f102cfcbf28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-16 10:20:00--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘weights/yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  1.03MB/s    in 79s     \n",
            "\n",
            "2023-07-16 10:21:20 (2.99 MB/s) - ‘weights/yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -P weights https://pjreddie.com/media/files/yolov3.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AArlXw7333zL",
        "outputId": "3575e644-212c-4a28-b74f-0538ece82f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-78e1eece1a3c>:40: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  return tf.layers.conv2d(\n",
            "<ipython-input-4-78e1eece1a3c>:3: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  return tf.layers.batch_normalization(\n",
            "<ipython-input-7-91a7ea47338e>:18: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ]
        }
      ],
      "source": [
        "batch_size = _BATCH_SIZE\n",
        "\n",
        "class_names = load_class_names('/content/coco.names')\n",
        "n_classes = len(class_names)\n",
        "max_output_size = 10\n",
        "iou_threshold = 0.5\n",
        "confidence_threshold = _CONF_THRESHOLD\n",
        "\n",
        "model = Yolo_v3(n_classes=n_classes,\n",
        "                model_size=_MODEL_SIZE,\n",
        "                max_output_size=max_output_size,\n",
        "                iou_threshold=iou_threshold,\n",
        "                confidence_threshold=confidence_threshold,\n",
        "                data_format = 'channels_last')\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\n",
        "detections = model(inputs, training=False)\n",
        "\n",
        "model_vars = tf.global_variables(scope='yolo_v3_model')\n",
        "assign_ops = load_weights(model_vars, './weights/yolov3.weights')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the whole video to the Yolo net, _BATCH_SIZE frame per time.\n",
        "\n",
        "Execution can be interrupted if the whole video is not necessary.\n",
        "\n",
        "Requests almost 1 min for 25 frames"
      ],
      "metadata": {
        "id": "FLOnVcGE7oB2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aKeuKdLtvPwz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "a0cf5521-25cc-4c1a-d8a9-f004e0c0b71c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5926\u001b[0m                  self).get_controller(default) as g, context.graph_mode():\n\u001b[0;32m-> 5927\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5928\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-acd4b9e07170>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0mcurr_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-acd4b9e07170>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MODEL_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mcurr_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exec_type, exec_value, exec_tb)\u001b[0m\n\u001b[1;32m   1651\u001b[0m       \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m       \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m       \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mclose_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         logging.error(\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "detection_result = []\n",
        "\n",
        "for i in range(0, total_frames, _BATCH_SIZE):\n",
        "  chosen_frames = frames[i:i+_BATCH_SIZE]\n",
        "\n",
        "  if len(chosen_frames) < 25:\n",
        "    break\n",
        "\n",
        "  batch = load_images(chosen_frames, model_size = _MODEL_SIZE)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "      sess.run(assign_ops)\n",
        "      curr_result = sess.run(detections, feed_dict={inputs: batch})\n",
        "      for res in curr_result:\n",
        "        detection_result.insert(len(detection_result), res)\n",
        "\n",
        "  print(\"\\rAnalyzed \", len(detection_result), \"/\", total_frames, \" frames\", end = \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL21QZgdInmK"
      },
      "outputs": [],
      "source": [
        "useful_indices = [0, 1, 2, 3, 5, 7, 9, 15, 16]\n",
        "boxed_frames = draw_boxes(frames, detection_result, useful_indices, _MODEL_SIZE)\n",
        "\n",
        "video_name = 'boxed_video.mp4'\n",
        "video_path = '/content/' + video_name\n",
        "\n",
        "width, height = boxed_frames[0].size\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video = cv2.VideoWriter(video_name, fourcc, frame_rate, (width, height))\n",
        "\n",
        "for image in boxed_frames:\n",
        "    video.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "video.release()\n",
        "\n",
        "from IPython.display import Video\n",
        "\n",
        "Video(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPXGeeeZ33zL"
      },
      "source": [
        "<a id=\"7\"></a>\n",
        "## 7. Detecting boxes direction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqmNegdo4OpO"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIzhNguYZUzN"
      },
      "outputs": [],
      "source": [
        "def getCenter(xy):\n",
        "  return ((xy[2] + xy[0]) /2, (xy[3] + xy[1]) /2)\n",
        "\n",
        "def distance(a, b):\n",
        "  return np.sqrt((b[1] - a[1])**2 + (b[0] - a[0])**2)\n",
        "\n",
        "def direction(a, b):\n",
        "  return np.arctan2(b[1] - a[1], b[0] - a[0]) % (2 * np.pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjbkORqcUm0c"
      },
      "outputs": [],
      "source": [
        "# detection_result is a dictionary list\n",
        "#   a dictionary for each frame\n",
        "# Its keys are class_names\n",
        "# Each element has 5 values:\n",
        "#   The first four are the box coordinates (x0, y0, x1, y1)\n",
        "#   the fifth is the confidence [0, 1]\n",
        "\n",
        "CONF = 4\n",
        "CLASS = 5\n",
        "TAG = 6\n",
        "\n",
        "MAX_AVG = 10\n",
        "MAX_DIST = 3\n",
        "UPDATE_GAP = 4\n",
        "CFT_FRAME = 1\n",
        "\n",
        "QUANTIZER = 5\n",
        "\n",
        "LENGTH_SCALE = 20 / CFT_FRAME\n",
        "LENGTH_CAP = 75 // QUANTIZER\n",
        "\n",
        "THICKNESS = 2\n",
        "\n",
        "param_text = \"Averaged frames: {:d}\\nUpdate gap: {:d}\\nCompared frame: {:d}\\nBox max distance: {:d}\\nQuantizer: {:d}\".format(MAX_AVG, UPDATE_GAP, CFT_FRAME, MAX_DIST, QUANTIZER)\n",
        "y0, dy = 12, 15\n",
        "\n",
        "safezone_size = boxed_frames[0].size[0] // 5\n",
        "\n",
        "saved_boxes = []\n",
        "\n",
        "for i, frame_box in enumerate(detection_result): # Frame dictionary\n",
        "  tmp = []\n",
        "\n",
        "  for cls in useful_indices:\n",
        "    boxes = frame_box[cls]\n",
        "\n",
        "    for box in boxes:\n",
        "      if box[CONF] > _CONF_THRESHOLD:\n",
        "        box = np.append(box, cls)\n",
        "        tmp.append(box)\n",
        "\n",
        "  saved_boxes.append(tmp)\n",
        "\n",
        "dict_history = {}\n",
        "index = 0\n",
        "\n",
        "# Now each box stores also the class value\n",
        "for i, saved_in_frame in enumerate(saved_boxes): # Frame by frame loop\n",
        "\n",
        "  if i == len(saved_boxes) -1:\n",
        "    break\n",
        "\n",
        "  for j, box in enumerate(saved_in_frame): # Frame's box loop\n",
        "\n",
        "    if len(box) < 7:\n",
        "      box = np.append(box, index)\n",
        "      dict_history[index] = []\n",
        "      index += 1\n",
        "\n",
        "    cft_frame = saved_boxes[i+1] # Frame to be compared\n",
        "\n",
        "    resize_factor = (boxed_frames[i].size[0] / _MODEL_SIZE[0], boxed_frames[i].size[1] / _MODEL_SIZE[1])\n",
        "\n",
        "    tmp_box = box[CONF:]\n",
        "    box = [box[i] * resize_factor[i % 2] // QUANTIZER for i in range(4)]\n",
        "\n",
        "    box = np.append(box, tmp_box)\n",
        "\n",
        "    saved_boxes[i][j] = box\n",
        "\n",
        "    centerBox = getCenter(box)\n",
        "\n",
        "    minDist = MAX_DIST\n",
        "\n",
        "    for k, cft_box in enumerate(cft_frame): # Loop on compared frame\n",
        "\n",
        "      if box[CLASS] != cft_box[CLASS]:\n",
        "        continue\n",
        "\n",
        "      cft_box = [cft_box[i] * resize_factor[i % 2] // QUANTIZER for i in range(4)]\n",
        "\n",
        "      centerCftBox = getCenter(cft_box)\n",
        "\n",
        "      dist = distance(centerBox, centerCftBox)\n",
        "\n",
        "      if dist < minDist:\n",
        "        minDist = dist\n",
        "        nearest_box = k\n",
        "\n",
        "    if minDist < MAX_DIST:\n",
        "      if len(saved_boxes[i+1][nearest_box]) < 7:\n",
        "        saved_boxes[i+1][nearest_box] = np.append(saved_boxes[i+1][nearest_box], box[TAG])\n",
        "\n",
        "# Now each box in saved_boxed stores a tag, to track the box between frames\n",
        "\n",
        "cft_found = False\n",
        "arrowed_frames = []\n",
        "draw_dist = [0 for i in range(index)]\n",
        "draw_dir = [0 for i in range(index)]\n",
        "\n",
        "car_speed = 0\n",
        "\n",
        "for i, saved_in_frame in enumerate(saved_boxes): # Frame by frame loop\n",
        "  curr_frame = frames[i]\n",
        "  na = np.array(curr_frame)\n",
        "\n",
        "  if i == len(saved_boxes) - CFT_FRAME -1:\n",
        "    break\n",
        "\n",
        "  for j, box in enumerate(saved_in_frame):\n",
        "\n",
        "    if box[CLASS] != 10:\n",
        "      continue\n",
        "\n",
        "    cft_frame = saved_boxes[i+CFT_FRAME] # Frame to be compared\n",
        "    curr_tag = int(box[TAG])\n",
        "\n",
        "    for cft_box in cft_frame: # Loop on compared frame\n",
        "\n",
        "      if len(cft_box) < 7:\n",
        "          continue\n",
        "\n",
        "      if cft_box[TAG] == curr_tag:\n",
        "        cft_found = True\n",
        "        break\n",
        "\n",
        "    if cft_found: # I found a near box, go on with direction calculation\n",
        "      centerBox = getCenter(box)\n",
        "      centerCftBox = getCenter(cft_box)\n",
        "\n",
        "      dist = distance(centerBox, centerCftBox) * LENGTH_SCALE\n",
        "      dir = direction(centerBox, centerCftBox)\n",
        "\n",
        "      if len(dict_history[curr_tag]) < MAX_AVG:\n",
        "        dict_history[curr_tag].append((dist, dir))\n",
        "      else:\n",
        "        dict_history[curr_tag].pop(0)\n",
        "        dict_history[curr_tag].append((dist, dir))\n",
        "\n",
        "      cmn_dist = 0\n",
        "      cmn_dir = 0\n",
        "\n",
        "      for val in dict_history[curr_tag]:\n",
        "        cmn_dist += val[0]\n",
        "        new_dir = val[1]\n",
        "\n",
        "        if abs(cmn_dir - new_dir) > np.pi:\n",
        "          if new_dir > cmn_dir:\n",
        "            new_dir -= 2*np.pi\n",
        "          else:\n",
        "            new_dir += 2*np.pi\n",
        "\n",
        "        cmn_dir += new_dir\n",
        "\n",
        "      cmn_dist /= len(dict_history[curr_tag])\n",
        "      cmn_dir /= len(dict_history[curr_tag])\n",
        "\n",
        "      car_speed = cmn_dist * np.sin(cmn_dir)\n",
        "      print(car_speed)\n",
        "\n",
        "  text = param_text + '\\nCar speed est.: {:d}'.format(car_speed)\n",
        "  for s, line in enumerate(text.split('\\n')):\n",
        "    y = y0 + s*dy\n",
        "    na = cv2.putText(na, text = line, org = (0, y), color=(255, 255, 255), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.5)\n",
        "\n",
        "  for j, box in enumerate(saved_in_frame): # Frame's box loop\n",
        "\n",
        "    if box[CLASS] == 10:\n",
        "      continue\n",
        "\n",
        "    cft_frame = saved_boxes[i+CFT_FRAME] # Frame to be compared\n",
        "    curr_tag = int(box[TAG])\n",
        "\n",
        "    for cft_box in cft_frame: # Compared frame's boxes loop\n",
        "\n",
        "      if len(cft_box) < 7:\n",
        "          continue\n",
        "\n",
        "      if cft_box[TAG] == curr_tag:\n",
        "        cft_found = True\n",
        "        break\n",
        "\n",
        "    if cft_found: # I found a near box, go on with direction calculation\n",
        "      centerBox = getCenter(box)\n",
        "      centerCftBox = getCenter(cft_box) + (0, car_speed)\n",
        "\n",
        "      dist = distance(centerBox, centerCftBox) * LENGTH_SCALE\n",
        "      dir = direction(centerBox, centerCftBox)\n",
        "\n",
        "      if len(dict_history[curr_tag]) < MAX_AVG:\n",
        "        dict_history[curr_tag].append((dist, dir))\n",
        "      else:\n",
        "        dict_history[curr_tag].pop(0)\n",
        "        dict_history[curr_tag].append((dist, dir))\n",
        "\n",
        "      avg_dist = 0\n",
        "      avg_dir = 0\n",
        "\n",
        "      for val in dict_history[curr_tag]:\n",
        "        avg_dist += val[0]\n",
        "        new_dir = val[1]\n",
        "\n",
        "        if abs(avg_dir - new_dir) > np.pi:\n",
        "          if new_dir > avg_dir:\n",
        "            new_dir -= 2*np.pi\n",
        "          else:\n",
        "            new_dir += 2*np.pi\n",
        "\n",
        "        avg_dir += new_dir\n",
        "\n",
        "      avg_dist /= len(dict_history[curr_tag])\n",
        "      avg_dir /= len(dict_history[curr_tag])\n",
        "\n",
        "      if avg_dist > LENGTH_CAP:\n",
        "        avg_dist = LENGTH_CAP\n",
        "\n",
        "      if i % UPDATE_GAP == 0:\n",
        "        draw_dist[curr_tag] = avg_dist\n",
        "        draw_dir[curr_tag] = avg_dir\n",
        "\n",
        "      start = (int(centerBox[0]) * QUANTIZER, int(centerBox[1]) * QUANTIZER)\n",
        "      end = (int(centerBox[0] + draw_dist[curr_tag] * np.cos(draw_dir[curr_tag])) * QUANTIZER,\n",
        "             int(centerBox[1] + draw_dist[curr_tag] * np.sin(draw_dir[curr_tag])) * QUANTIZER)\n",
        "\n",
        "      if avg_dist < 1:\n",
        "        red = 0\n",
        "        green = 255\n",
        "      elif centerBox[0] * QUANTIZER < safezone_size and start[0] > end[0]:\n",
        "        # I'm in the left side and the arrow is pointing left\n",
        "        red = 0\n",
        "        green = 255\n",
        "      elif centerBox[0] * QUANTIZER > boxed_frames[i].size[0] - safezone_size and start[0] < end[0]:\n",
        "        # I'm in the right side and the arrow is pointing right\n",
        "        red = 0\n",
        "        green = 255\n",
        "      else:\n",
        "        red = int(np.abs(np.cos(draw_dir[curr_tag])*255))\n",
        "        green = int(np.abs(np.sin(draw_dir[curr_tag])*255))\n",
        "\n",
        "      curr_frame = cv2.arrowedLine(na, start, end, (red, green, 0), THICKNESS)\n",
        "\n",
        "      xy = box[:4] * QUANTIZER\n",
        "\n",
        "      for t in np.linspace(0, 1, THICKNESS):\n",
        "          curr_frame = cv2.rectangle(curr_frame, (int(xy[0]), int(xy[1])), (int(xy[2]), int(xy[3])), color=(red, green, 0), thickness = THICKNESS)\n",
        "\n",
        "      curr_frame = cv2.rectangle(curr_frame, (safezone_size, 0), (safezone_size, boxed_frames[i].size[1]), color=(255, 255, 255), thickness = THICKNESS)\n",
        "      curr_frame = cv2.rectangle(curr_frame, (boxed_frames[i].size[0] - safezone_size, 0), (boxed_frames[i].size[0] - safezone_size, boxed_frames[i].size[1]), color=(255, 255, 255), thickness = THICKNESS)\n",
        "\n",
        "      cv2.putText(curr_frame, text = \"Id: \" + str(curr_tag), org = (int(xy[0]), int(xy[1])), color=(255, 255, 255), fontFace = 1, fontScale = 1)\n",
        "\n",
        "      cft_found = False\n",
        "\n",
        "  img = Image.fromarray(np.array(curr_frame))\n",
        "  arrowed_frames.append(img)\n",
        "\n",
        "print(\"Last frame:\")\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJBxmnQ3gqfl"
      },
      "outputs": [],
      "source": [
        "video_name = 'arrowed_video.mp4'\n",
        "video_path = '/content/' + video_name\n",
        "\n",
        "width, height = boxed_frames[0].size\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video = cv2.VideoWriter(video_name, fourcc, frame_rate, (width, height))\n",
        "\n",
        "for image in arrowed_frames:\n",
        "    video.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "video.release()\n",
        "\n",
        "from IPython.display import Video\n",
        "\n",
        "Video(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optical flow**"
      ],
      "metadata": {
        "id": "eqeGK2TEVMyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optical_flow_rect(img1, img2, boxes):\n",
        "\n",
        "  img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "  img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Whole optical flow\n",
        "  flow = cv2.calcOpticalFlowFarneback(img1_gray, img2_gray, None,  pyr_scale=0.8, levels=15, winsize=5,\n",
        "                                      iterations=5, poly_n=5, poly_sigma=0,\n",
        "                                      flags=10)\n",
        "\n",
        "  # Filter the optical flow in the box\n",
        "  for box in boxes:\n",
        "    x0, y0, x1, y1 = [int(box[j]) for j in range(4)]\n",
        "\n",
        "    box_flow = flow[y0:y1, x0:x1, :].copy()\n",
        "\n",
        "    mag, ang = cv2.cartToPolar(box_flow[..., 0], box_flow[..., 1])\n",
        "\n",
        "    try:\n",
        "      avg_dist = np.mean(np.mean(mag)) * 50\n",
        "    except:\n",
        "      avg_dist = 0\n",
        "\n",
        "    if avg_dist > LENGTH_CAP:\n",
        "        avg_dist = LENGTH_CAP\n",
        "\n",
        "    try:\n",
        "      avg_dir = 0\n",
        "\n",
        "      for row in ang:\n",
        "        for val in row:\n",
        "          new_dir = val\n",
        "\n",
        "          if abs(avg_dir - new_dir) > np.pi:\n",
        "            if new_dir > avg_dir:\n",
        "              new_dir -= 2*np.pi\n",
        "            else:\n",
        "              new_dir += 2*np.pi\n",
        "\n",
        "          avg_dir += new_dir\n",
        "\n",
        "      avg_dir /= ang.size\n",
        "    except:\n",
        "      avg_dir = 0\n",
        "\n",
        "    centerX = (x0 + x1) // 2\n",
        "    centerY = (y0 + y1) // 2\n",
        "\n",
        "    start = (centerX, centerY)\n",
        "    end = (int(centerX + avg_dist * np.cos(avg_dir)),\n",
        "            int(centerY + avg_dist * np.sin(avg_dir)))\n",
        "\n",
        "    if avg_dist < 1:\n",
        "      red = 0\n",
        "      green = 255\n",
        "    else:\n",
        "      red = int(np.abs(np.cos(avg_dir)*255))\n",
        "      green = int(np.abs(np.sin(avg_dir)*255))\n",
        "\n",
        "    try:\n",
        "      if ang is not None:\n",
        "        hsv = np.zeros((y1 - y0, x1 - x0, 3), dtype=np.uint8)\n",
        "\n",
        "        hsv[..., 0] = ang * 180 / np.pi / 2\n",
        "        hsv[..., 1] = 200\n",
        "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        colored_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "        img1[y0:y1, x0:x1] = colored_flow\n",
        "      else:\n",
        "        print(\"\\nAng is not valid: \", type(ang))\n",
        "    except:\n",
        "      print(\"\")\n",
        "\n",
        "    img1 = cv2.arrowedLine(img1, start, end, (red, green, 0), 2)\n",
        "\n",
        "  return img1"
      ],
      "metadata": {
        "id": "auaZmZ3GVL9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost 5 frame per second"
      ],
      "metadata": {
        "id": "9LYirxA6w3on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flowed_frames = []\n",
        "\n",
        "for i, saved_in_frame in enumerate(saved_boxes): # Frame by frame loop\n",
        "\n",
        "  if i == len(saved_boxes) - 1:\n",
        "    break\n",
        "\n",
        "  curr_frame = frames[i].copy()\n",
        "  next_frame = frames[i+1].copy()\n",
        "  resize_factor = (frames[i].shape[1] / _MODEL_SIZE[0], frames[i].shape[0] / _MODEL_SIZE[1])\n",
        "\n",
        "  boxes = []\n",
        "\n",
        "  for box in saved_in_frame:\n",
        "    box = [box[i] * QUANTIZER for i in range(4)]\n",
        "    boxes.append(box)\n",
        "\n",
        "  if len(boxes) == 0:\n",
        "    continue\n",
        "\n",
        "  curr_frame = optical_flow_rect(curr_frame, next_frame, boxes)\n",
        "\n",
        "  img = Image.fromarray(np.array(curr_frame))\n",
        "  flowed_frames.append(img)\n",
        "\n",
        "  print(\"\\rAnalyzed \", len(flowed_frames), \"/\", len(frames), \" frames\", end = \"\")\n",
        "\n",
        "print(\"\\nLast frame:\")\n",
        "display(Image.fromarray(curr_frame))"
      ],
      "metadata": {
        "id": "hWSIi2C5Y4L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_name = 'flowed_video.mp4'\n",
        "video_path = '/content/' + video_name\n",
        "\n",
        "width, height = boxed_frames[0].size\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video = cv2.VideoWriter(video_name, fourcc, frame_rate, (width, height))\n",
        "\n",
        "for image in flowed_frames:\n",
        "    video.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "video.release()\n",
        "\n",
        "from IPython.display import Video\n",
        "\n",
        "Video(video_path)"
      ],
      "metadata": {
        "id": "cFuSxIoOl4Er"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}